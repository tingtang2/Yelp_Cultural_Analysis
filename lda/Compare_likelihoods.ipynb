{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import lda\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import gensim \n",
    "import sklearn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sparse.load_npz('/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/X_train.npz')\n",
    "cc= np.load(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/y_train.npy\")\n",
    "locs = []\n",
    "with open(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/locs_train.json\", 'r') as file:\n",
    "    locs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = X_train.getnnz(1)>0\n",
    "X_train = X_train[ix]\n",
    "cc= cc[ix]\n",
    "locs= np.array(locs)[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lda.LDA(n_topics= 25, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all zero column in document-term matrix found\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished initializing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [15:53:53<00:00, 57.23s/it] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lda.LDA at 0x1088f7ba8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first = 10000\n",
    "\n",
    "model.fit(X_train.astype(np.int32), cc.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_ndz\", model.ndz_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_nzw\", model.nzw_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_nzwc\", model.nzwc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_nx\", model.nx_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_nz\", model.nz_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_nzc\", model.nzc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccLDA_nzw = np.load(\"ccLDA_nzw.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccLDA_nzwc = np.load(\"ccLDA_nzwc.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccLDA_nx = np.load(\"ccLDA_nx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccLDA_nzc = np.load(\"ccLDA_nzc.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccLDA_nz = np.load(\"ccLDA_nz.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm_ndz = np.load(\"./model_persistence/all_eths_less_than_15000_1000_its_25_topics/ndz.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm_nzw = np.load(\"./model_persistence/all_eths_less_than_15000_1000_its_25_topics/nzw.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate P(z|d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = sparse.load_npz('/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/X_val.npz')\n",
    "cc_val= np.load(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/y_val.npy\")\n",
    "locs_val = []\n",
    "with open(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/locs_val.json\", 'r') as file:\n",
    "    locs_val = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sparse.load_npz('/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/X_test.npz')\n",
    "cc_test= np.load(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/y_test.npy\")\n",
    "locs_test = []\n",
    "with open(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/locs_test.json\", 'r') as file:\n",
    "    locs_test = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_X = sparse.vstack([X_val, X_test])\n",
    "total_c = np.concatenate((cc_val, cc_test))\n",
    "total_locs = locs_val + locs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = total_X.getnnz(1)>0\n",
    "total_X = total_X[ix]\n",
    "total_c= total_c[ix]\n",
    "total_locs= np.array(total_locs)[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1430699, 822218)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1430699,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1430699"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ccLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_X = total_X.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_c = total_c.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all zero column in document-term matrix found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fd9790eb0aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_to_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_X\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# word and document indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mCS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_c\u001b[0m \u001b[0;31m# ethnic cuisine for each review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Yelp_Cultural_Analysis/lda/utils.py\u001b[0m in \u001b[0;36mmatrix_to_lists\u001b[0;34m(doc_word)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# if doc_word is a scipy sparse matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mdoc_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0msparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mtolil\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "D, W = total_X.shape  # documents and vocab size\n",
    "N = int(total_X.sum()) # number of total tokens\n",
    "C = len(set(total_c)) # number of collections\n",
    "n_topics = 25\n",
    "#n_regions = self.n_regions\n",
    "n_iter = 1000\n",
    "\n",
    "# for background distribution (normal LDA)\n",
    "ndz_ = np.zeros((D, n_topics), dtype=np.intc)\n",
    "\n",
    "\n",
    "WS, DS = utils.matrix_to_lists(total_X) # word and document indices\n",
    "CS = total_c # ethnic cuisine for each review\n",
    "\n",
    "ZS = np.empty_like(WS, dtype=np.intc) # topics for each word\n",
    "\n",
    "# TO DO: check if initializing xs as zeros or random is better\n",
    "XS = np.random.binomial(np.ones(WS.shape[0], dtype=np.intc), .5) # indicator for background\n",
    "#self.XS = XS = np.zeros(self.WS.shape[0], dtype=np.intc) # indicator for background\n",
    "XS = XS.astype('intc')\n",
    "\n",
    "\n",
    "np.testing.assert_equal(N, len(WS))\n",
    "np.testing.assert_equal(len(set(DS)), len(CS))\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    w, d= WS[i], DS[i]\n",
    "\n",
    "    z_new = i % n_topics\n",
    "    ZS[i] = z_new\n",
    "\n",
    "    ndz_[d, z_new] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _compare_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-294362961.8238259"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_compare_likelihoods._loglikelihood(ndz_, nd, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = np.sum(ndz_, axis=1).astype(np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = utils.check_random_state(None)\n",
    "rands = rng.rand(1024**2 // 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = utils.check_random_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# held out inference for ccLDA\n",
    "\n",
    "n_topics, vocab_size = ccLDA_nzw.shape\n",
    "\n",
    "alpha = np.repeat(0.1, n_topics).astype(np.float64)\n",
    "beta = np.repeat(0.01, vocab_size).astype(np.float64)\n",
    "delta = np.repeat(0.01, vocab_size).astype(np.float64) # for cross collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [4:14:53<00:00, 15.29s/it] \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total= 1000) as pbar:\n",
    "    for i in range(1000):    \n",
    "        random_state.shuffle(rands)\n",
    "\n",
    "        _compare_likelihoods._sample_topics_infer(WS, DS, ZS, CS, XS,\n",
    "             ccLDA_nx, ccLDA_nzw, ndz_, ccLDA_nz, ccLDA_nzwc, ccLDA_nzc, alpha,\n",
    "             beta, delta, 1.0, 1.0, rands)\n",
    "        \n",
    "        if i % 10 ==0:\n",
    "            #print(\"\\rit: \" +str(i), end=\"\")\n",
    "            #likelihoods.append(_compare_likelihoods._loglikelihood(ndz_, nd, 0.1))\n",
    "            pbar.update(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"cclda_likelihoods\", np.array(likelihoods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1430699, 25)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndz_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all zero column in document-term matrix found\n"
     ]
    }
   ],
   "source": [
    "D, W = total_X.shape  # documents and vocab size\n",
    "n_topics = 25\n",
    "#n_regions = self.n_regions\n",
    "n_iter = 1000\n",
    "N = int(total_X.sum()) # number of total tokens\n",
    "\n",
    "# for background distribution (normal LDA)\n",
    "ndz_ = np.zeros((D, n_topics), dtype=np.intc)\n",
    "\n",
    "\n",
    "WS, DS = utils.matrix_to_lists(total_X) # word and document indices\n",
    "\n",
    "ZS = np.empty_like(WS, dtype=np.intc) # topics for each word\n",
    "\n",
    "\n",
    "\n",
    "np.testing.assert_equal(N, len(WS))\n",
    "\n",
    "for i in range(N):\n",
    "    w, d= WS[i], DS[i]\n",
    "\n",
    "    z_new = i % n_topics\n",
    "    ZS[i] = z_new\n",
    "\n",
    "    ndz_[d, z_new] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _compare_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = np.sum(ndz_, axis=1).astype(np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = np.sum(htm_ndz, axis=0).astype(np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = ccLDA_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = utils.check_random_state(None)\n",
    "rands = rng.rand(1024**2 // 8) \n",
    "random_state = utils.check_random_state(None)\n",
    "likelihoods = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# held out inference for ccLDA\n",
    "\n",
    "n_topics, vocab_size = htm_nzw.shape\n",
    "\n",
    "alpha = np.repeat(0.1, n_topics).astype(np.float64)\n",
    "beta = np.repeat(0.01, vocab_size).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics, vocab_size = ccLDA_nzw.shape\n",
    "\n",
    "alpha = np.repeat(0.1, n_topics).astype(np.float64)\n",
    "beta = np.repeat(0.01, vocab_size).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [22:16<00:00,  6.68s/it]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total= 200) as pbar:\n",
    "    for i in range(200):    \n",
    "        random_state.shuffle(rands)\n",
    "\n",
    "        _compare_likelihoods._sample_topics_infer_base(WS, DS, ZS, htm_nzw, ndz_, nz, alpha,\n",
    "             beta, rands)\n",
    "        \n",
    "        if i % 10 ==0:\n",
    "            #print(\"\\rit: \" +str(i), end=\"\")\n",
    "            likelihoods.append(_compare_likelihoods._loglikelihood(ndz_, nd, 0.1))\n",
    "            pbar.update(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [21:34<00:00,  6.47s/it]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total= 200) as pbar:\n",
    "    for i in range(200):    \n",
    "        random_state.shuffle(rands)\n",
    "\n",
    "        _compare_likelihoods._sample_topics_infer_base(WS, DS, ZS, ccLDA_nzw, ndz_, nz, alpha,\n",
    "             beta, rands)\n",
    "        \n",
    "        if i % 10 ==0:\n",
    "            #print(\"\\rit: \" +str(i), end=\"\")\n",
    "            likelihoods.append(_compare_likelihoods._loglikelihood(ndz_, nd, 0.1))\n",
    "            pbar.update(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"htm_likelihoods_200_its\", likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_likelihoods_200_its\", likelihoods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

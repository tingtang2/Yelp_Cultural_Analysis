{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import _compare_likelihoods\n",
    "import lda\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import gensim \n",
    "import sklearn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get previous ccLDA params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccLDA_nzw = np.load(\"ccLDA_nzw.npy\")\n",
    "ccLDA_nx = np.load(\"ccLDA_nx.npy\")\n",
    "ccLDA_nz = np.load(\"ccLDA_nz.npy\")\n",
    "ccLDA_nzwc = np.load(\"ccLDA_nzwc.npy\")\n",
    "ccLDA_nzc = np.load(\"ccLDA_nzc.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = sparse.load_npz('/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/X_val.npz')\n",
    "cc_val= np.load(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/y_val.npy\")\n",
    "locs_val = []\n",
    "with open(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/locs_val.json\", 'r') as file:\n",
    "    locs_val = json.load(file)\n",
    "    \n",
    "X_test = sparse.load_npz('/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/X_test.npz')\n",
    "cc_test= np.load(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/y_test.npy\")\n",
    "locs_test = []\n",
    "with open(\"/Users/tingchen/Desktop/Yelp Project/data_n_20_removed_all_eths_less_than_15000/locs_test.json\", 'r') as file:\n",
    "    locs_test = json.load(file)\n",
    "    \n",
    "total_X = sparse.vstack([X_val, X_test])\n",
    "total_c = np.concatenate((cc_val, cc_test))\n",
    "total_locs = locs_val + locs_test\n",
    "\n",
    "ix = total_X.getnnz(1)>0\n",
    "total_X = total_X[ix]\n",
    "total_c= total_c[ix]\n",
    "total_locs= np.array(total_locs)[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up inference for ccLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all zero column in document-term matrix found\n"
     ]
    }
   ],
   "source": [
    "total_X = total_X.astype(np.int32)\n",
    "total_c = total_c.astype(np.int32)\n",
    "\n",
    "D, W = total_X.shape  # documents and vocab size\n",
    "N = int(total_X.sum()) # number of total tokens\n",
    "C = len(set(total_c)) # number of collections\n",
    "n_topics = 25\n",
    "#n_regions = self.n_regions\n",
    "n_iter = 1000\n",
    "\n",
    "# for background distribution (normal LDA)\n",
    "ndz_ = np.zeros((D, n_topics), dtype=np.intc)\n",
    "\n",
    "\n",
    "WS, DS = utils.matrix_to_lists(total_X) # word and document indices\n",
    "CS = total_c # ethnic cuisine for each review\n",
    "\n",
    "ZS = np.empty_like(WS, dtype=np.intc) # topics for each word\n",
    "\n",
    "# TO DO: check if initializing xs as zeros or random is better\n",
    "XS = np.random.binomial(np.ones(WS.shape[0], dtype=np.intc), .5) # indicator for background\n",
    "#self.XS = XS = np.zeros(self.WS.shape[0], dtype=np.intc) # indicator for background\n",
    "XS = XS.astype('intc')\n",
    "\n",
    "\n",
    "np.testing.assert_equal(N, len(WS))\n",
    "np.testing.assert_equal(len(set(DS)), len(CS))\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    w, d= WS[i], DS[i]\n",
    "\n",
    "    z_new = i % n_topics\n",
    "    ZS[i] = z_new\n",
    "\n",
    "    ndz_[d, z_new] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do sampling for document level counts only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = np.sum(ndz_, axis=1).astype(np.intc)\n",
    "rng = utils.check_random_state(None)\n",
    "rands = rng.rand(1024**2 // 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = utils.check_random_state(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics, vocab_size = ccLDA_nzw.shape\n",
    "\n",
    "alpha = np.repeat(0.1, n_topics).astype(np.float64)\n",
    "beta = np.repeat(0.01, vocab_size).astype(np.float64)\n",
    "delta = np.repeat(0.01, vocab_size).astype(np.float64) # for cross collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 620/1000 [2:59:43<1:50:09, 17.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-546fa170109b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         _compare_likelihoods._sample_topics_infer(WS, DS, ZS, CS, XS,\n\u001b[1;32m      6\u001b[0m              \u001b[0mccLDA_nx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccLDA_nzw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndz_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccLDA_nz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccLDA_nzwc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mccLDA_nzc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m              beta, delta, 1.0, 1.0, rands)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total= 1000) as pbar:\n",
    "    for i in range(1000):    \n",
    "        random_state.shuffle(rands)\n",
    "\n",
    "        _compare_likelihoods._sample_topics_infer(WS, DS, ZS, CS, XS,\n",
    "             ccLDA_nx, ccLDA_nzw, ndz_, ccLDA_nz, ccLDA_nzwc, ccLDA_nzc, alpha,\n",
    "             beta, delta, 1.0, 1.0, rands)\n",
    "        \n",
    "        if i % 10 ==0:\n",
    "            #print(\"\\rit: \" +str(i), end=\"\")\n",
    "            #likelihoods.append(_compare_likelihoods._loglikelihood(ndz_, nd, 0.1))\n",
    "            pbar.update(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ccLDA_eval_ndz\",ndz_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do c3rLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "htm_ndz = np.load(\"./model_persistence/all_eths_less_than_15000_1000_its_25_topics/ndz.npy\")\n",
    "htm_nzw = np.load(\"./model_persistence/all_eths_less_than_15000_1000_its_25_topics/nzw.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all zero column in document-term matrix found\n"
     ]
    }
   ],
   "source": [
    "D, W = total_X.shape  # documents and vocab size\n",
    "n_topics = 25\n",
    "#n_regions = self.n_regions\n",
    "n_iter = 1000\n",
    "N = int(total_X.sum()) # number of total tokens\n",
    "\n",
    "# for background distribution (normal LDA)\n",
    "ndz_ = np.zeros((D, n_topics), dtype=np.intc)\n",
    "\n",
    "\n",
    "WS, DS = utils.matrix_to_lists(total_X) # word and document indices\n",
    "\n",
    "ZS = np.empty_like(WS, dtype=np.intc) # topics for each word\n",
    "\n",
    "\n",
    "\n",
    "np.testing.assert_equal(N, len(WS))\n",
    "\n",
    "for i in range(N):\n",
    "    w, d= WS[i], DS[i]\n",
    "\n",
    "    z_new = i % n_topics\n",
    "    ZS[i] = z_new\n",
    "\n",
    "    ndz_[d, z_new] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = np.sum(ndz_, axis=1).astype(np.intc)\n",
    "nz = np.sum(htm_ndz, axis=0).astype(np.intc)\n",
    "\n",
    "rng = utils.check_random_state(None)\n",
    "rands = rng.rand(1024**2 // 8) \n",
    "random_state = utils.check_random_state(None)\n",
    "\n",
    "\n",
    "n_topics, vocab_size = htm_nzw.shape\n",
    "\n",
    "alpha = np.repeat(0.1, n_topics).astype(np.float64)\n",
    "beta = np.repeat(0.01, vocab_size).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [52:19<00:00,  6.28s/it]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total= 500) as pbar:\n",
    "    for i in range(500):    \n",
    "        random_state.shuffle(rands)\n",
    "\n",
    "        _compare_likelihoods._sample_topics_infer_base(WS, DS, ZS, htm_nzw, ndz_, nz, alpha,\n",
    "             beta, rands)\n",
    "        \n",
    "        if i % 10 ==0:\n",
    "            #print(\"\\rit: \" +str(i), end=\"\")\n",
    "            pbar.update(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"c3rLDA_eval_ndz\",ndz_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

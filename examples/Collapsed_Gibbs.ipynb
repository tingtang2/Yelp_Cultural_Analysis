{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapsed Gibbs Sampling from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From \"Parameter estimation for text analysis\", http://www.arbylon.net/publications/text-est.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = np.array([\n",
    "                       [0, 0, 1, 2, 2],\n",
    "                       [0, 0, 1, 1, 1],\n",
    "                       [0, 1, 2, 2, 2],\n",
    "                       [4, 4, 4, 4, 4],\n",
    "                       [3, 3, 4, 4, 4],\n",
    "                       [3, 4, 4, 4, 4]])\n",
    "\n",
    "n_topics = 4\n",
    "n_docs, n_words = dummy_input.shape\n",
    "doc_lens = np.sum(dummy_input, axis=1)\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(n_docs):\n",
    "    doc = []\n",
    "    for j in range(n_words):\n",
    "        doc += [j] *dummy_input[i, j]\n",
    "    \n",
    "    data += [doc]\n",
    "\n",
    "beta = 1\n",
    "alpha = np.ones(n_topics) * 1\n",
    "\n",
    "# initialize topic assignments of words in each doc\n",
    "\n",
    "topic_assignments = {}\n",
    "for doc in range(n_docs):\n",
    "    topic_assignments[doc] = [0] * doc_lens[doc]\n",
    "\n",
    "cntTW = np.zeros((n_topics, n_words))          \n",
    "cntDT = np.zeros((n_docs, n_topics))             \n",
    "cntT = np.zeros(n_topics)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 3, 3, 4, 4]), list([2, 3, 4]),\n",
       "       list([1, 2, 2, 3, 3, 4, 4]),\n",
       "       list([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]),\n",
       "       list([0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]),\n",
       "       list([0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(n_docs):\n",
    "    i = 0 # index for word position\n",
    "    \n",
    "    for word in data[doc]:\n",
    "        topic = np.random.randint(0, n_topics-1)\n",
    "        topic_assignments[doc][i] = topic\n",
    "        cntTW[topic, word] += 1\n",
    "        cntDT[doc, topic] += 1\n",
    "        cntT[topic] += 1\n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogLikelihood():                                        # FIND (JOINT) LOG-LIKELIHOOD VALUE\n",
    "    l = 0\n",
    "    for z in range(n_topics):                                # log p(w|z,\\beta)\n",
    "        l += gammaln(n_words * beta)\n",
    "        l -= n_words * gammaln(beta)\n",
    "        l += np.sum(gammaln(cntTW[z] + beta))\n",
    "        l -= gammaln(np.sum(cntTW[z] + beta))\n",
    "    for doc in range(n_docs):                                  # log p(z|\\alpha)\n",
    "        l += gammaln(np.sum(alpha))\n",
    "        l -= np.sum(gammaln(alpha))\n",
    "        l += np.sum(gammaln(cntDT[doc] + alpha))\n",
    "        l -= gammaln(np.sum(cntDT[doc] + alpha))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findThetaPhi():\n",
    "    th = np.zeros((n_docs, n_topics))                     # SPACE FOR THETA\n",
    "    ph = np.zeros((n_topics, n_words))                   # SPACE FOR PHI\n",
    "    for d in range(n_docs):\n",
    "        for z in range(n_topics):\n",
    "            th[d][z] = (cntDT[d][z] + alpha[z]) / (doc_lens[d] + np.sum(alpha))\n",
    "    for z in range(n_topics):\n",
    "        for w in range(n_words):\n",
    "            ph[z][w] = (cntTW[z][w] + beta) / (cntT[z] + beta * n_words)\n",
    "    return ph, th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STATE\n",
      "\tLikelihood: -212.46198132591644\n",
      "\tLikelihood: -208.8918553448105\n",
      "\tLikelihood: -202.944118158491\n",
      "\tLikelihood: -203.637363098979\n",
      "\tLikelihood: -201.19642732692444\n",
      "\tLikelihood: -218.18349515952139\n",
      "\tLikelihood: -210.97315045513966\n",
      "\tLikelihood: -192.78878040093895\n",
      "\tLikelihood: -191.77690763880872\n",
      "\tLikelihood: -217.42072921642864\n",
      "\tLikelihood: -207.67166829600137\n",
      "\tLikelihood: -205.18869938176402\n",
      "\tLikelihood: -209.20745222457663\n",
      "\tLikelihood: -211.08862789542263\n",
      "\tLikelihood: -201.41804441698162\n",
      "\tLikelihood: -216.5675986457219\n",
      "\tLikelihood: -197.9115467172603\n",
      "\tLikelihood: -189.44640143505865\n",
      "\tLikelihood: -195.52678738039987\n",
      "\tLikelihood: -193.76851509674748\n",
      "\tLikelihood: -212.92523920724284\n",
      "\tLikelihood: -204.07416881696463\n",
      "\tLikelihood: -208.97366923962448\n",
      "\tLikelihood: -195.18222896900522\n",
      "\tLikelihood: -208.12216808712822\n",
      "\tLikelihood: -200.70608329139674\n",
      "\tLikelihood: -211.83459508822392\n",
      "\tLikelihood: -204.9999528248136\n",
      "\tLikelihood: -201.25516925741323\n",
      "\tLikelihood: -209.46859217352602\n",
      "\tLikelihood: -200.64882709229562\n",
      "\tLikelihood: -199.48704848493293\n",
      "\tLikelihood: -201.6278341382228\n",
      "\tLikelihood: -201.10168590387292\n",
      "\tLikelihood: -201.29464665536705\n",
      "\tLikelihood: -220.8200921405823\n",
      "\tLikelihood: -214.94711089710125\n",
      "\tLikelihood: -212.05477859790966\n",
      "\tLikelihood: -210.55563643632013\n",
      "\tLikelihood: -205.4442748423955\n",
      "\tLikelihood: -212.27156997456433\n",
      "\tLikelihood: -200.90098255481638\n",
      "\tLikelihood: -199.96679796459853\n",
      "\tLikelihood: -205.2631371278287\n",
      "\tLikelihood: -202.1483438631729\n",
      "\tLikelihood: -209.31538174008512\n",
      "\tLikelihood: -212.15642847856208\n",
      "\tLikelihood: -202.59247236849933\n",
      "\tLikelihood: -195.82048024653068\n",
      "\tLikelihood: -215.31449552843895\n",
      "\tLikelihood: -207.4453209033132\n",
      "\tLikelihood: -204.28987905387942\n",
      "\tLikelihood: -207.87313172977406\n",
      "\tLikelihood: -215.19487636214757\n",
      "\tLikelihood: -211.32241070419644\n",
      "\tLikelihood: -207.4019189767904\n",
      "\tLikelihood: -197.1638366637128\n",
      "\tLikelihood: -207.22637074738913\n",
      "\tLikelihood: -204.6523088503218\n",
      "\tLikelihood: -215.43254352208623\n",
      "\tLikelihood: -205.5283982911311\n",
      "\tLikelihood: -207.82498570761203\n",
      "\tLikelihood: -215.9448648009958\n",
      "\tLikelihood: -209.71350456958993\n",
      "\tLikelihood: -216.0329274305633\n",
      "\tLikelihood: -214.09145046320168\n",
      "\tLikelihood: -208.71572575826445\n",
      "\tLikelihood: -199.1016023643631\n",
      "\tLikelihood: -206.0584749024165\n",
      "\tLikelihood: -215.6901226085809\n",
      "\tLikelihood: -213.4354769735254\n",
      "\tLikelihood: -197.29549107332804\n",
      "\tLikelihood: -203.12882093214978\n",
      "\tLikelihood: -204.87829941128302\n",
      "\tLikelihood: -213.1476928654631\n",
      "\tLikelihood: -210.45525820767153\n",
      "\tLikelihood: -194.70716273572293\n",
      "\tLikelihood: -208.33459849475668\n",
      "\tLikelihood: -202.40846223215033\n",
      "\tLikelihood: -209.44986965692095\n",
      "\tLikelihood: -204.1672289732948\n",
      "\tLikelihood: -195.60023113421937\n",
      "\tLikelihood: -205.69667918165283\n",
      "\tLikelihood: -210.14737947474714\n",
      "\tLikelihood: -202.6808320424771\n",
      "\tLikelihood: -205.95807352937584\n",
      "\tLikelihood: -212.7388932471847\n",
      "\tLikelihood: -212.7624295929275\n",
      "\tLikelihood: -203.92516506762678\n",
      "\tLikelihood: -203.55510188656956\n",
      "\tLikelihood: -201.59686772735296\n",
      "\tLikelihood: -211.47726464431764\n",
      "\tLikelihood: -207.3323193253985\n",
      "\tLikelihood: -205.77862905770132\n",
      "\tLikelihood: -211.95130343275054\n",
      "\tLikelihood: -205.22912860350704\n",
      "\tLikelihood: -204.68302881950325\n",
      "\tLikelihood: -208.67823827391442\n",
      "\tLikelihood: -206.1887016784557\n",
      "\tLikelihood: -199.65841622593643\n"
     ]
    }
   ],
   "source": [
    "# COLLAPSED GIBBS SAMPLING\n",
    "print(\"INITIAL STATE\")\n",
    "print(\"\\tLikelihood:\", LogLikelihood())               # FIND (JOINT) LOG-LIKELIHOOD\n",
    "\n",
    "SAMPLES = 0\n",
    "total_samples = 1000\n",
    "\n",
    "\n",
    "burnin = 2000\n",
    "interval = 10\n",
    "\n",
    "phis = []\n",
    "thetas = []\n",
    "\n",
    "for s in range(burnin + total_samples):\n",
    "    for doc in range(n_docs):\n",
    "        i = 0 # index for word position\n",
    "        for word in data[doc]:\n",
    "            # decrement counts and sums for current assignment\n",
    "            topic = topic_assignments[doc][i]\n",
    "            cntTW[topic, word] -= 1\n",
    "            cntDT[doc, topic] -= 1\n",
    "            cntT[topic] -= 1    \n",
    "            \n",
    "            # calculate full conditional and sample new topic\n",
    "            prL = (cntDT[doc] + alpha) / (doc_lens[doc] - 1 + np.sum(alpha))\n",
    "            prR = (cntTW[:,word] + beta) / (cntT + beta * n_words)\n",
    "            \n",
    "            \n",
    "            prFullCond = prL * prR  \n",
    "            prFullCond = np.asarray(prFullCond)\n",
    "            prFullCond /= np.sum(prFullCond)\n",
    "            #print(prFullCond)\n",
    "            #print(np.sum(prFullCond))\n",
    "             \n",
    "            new_topic = np.random.multinomial(1, prFullCond.astype(np.float64)).argmax()\n",
    "            \n",
    "            # update counts\n",
    "            topic_assignments[doc][i] = new_topic\n",
    "            cntTW[new_topic, word] += 1\n",
    "            cntDT[doc, new_topic] += 1\n",
    "            cntT[new_topic] += 1\n",
    "            i+=1\n",
    "            \n",
    "      \n",
    "    if s > burnin and s % interval == 0:                    # FIND PHI AND THETA AFTER BURN-IN POINT\n",
    "        ph, th = findThetaPhi()\n",
    "        thetas.append(th)\n",
    "        phis.append(ph)\n",
    "        print(\"\\tLikelihood:\", LogLikelihood())\n",
    "        SAMPLES += 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
